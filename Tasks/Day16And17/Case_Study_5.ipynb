{"cells":[{"cell_type":"code","execution_count":15,"id":"3cb6fba2","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+------+-------+------+---------+----+\n","|userId|movieId|rating|timestamp|year|\n","+------+-------+------+---------+----+\n","|     1|     17|   4.0|944249077|1999|\n","|     1|     25|   1.0|944250228|1999|\n","|     1|     29|   2.0|943230976|1999|\n","|     1|     30|   5.0|944249077|1999|\n","|     1|     32|   5.0|943228858|1999|\n","|     1|     34|   2.0|943228491|1999|\n","|     1|     36|   1.0|944249008|1999|\n","|     1|     80|   5.0|944248943|1999|\n","|     1|    110|   3.0|943231119|1999|\n","|     1|    111|   5.0|944249008|1999|\n","|     1|    161|   1.0|943231162|1999|\n","|     1|    166|   5.0|943228442|1999|\n","|     1|    176|   4.0|944079496|1999|\n","|     1|    223|   3.0|944082810|1999|\n","|     1|    232|   5.0|943228442|1999|\n","|     1|    260|   5.0|943228696|1999|\n","|     1|    302|   4.0|944253272|1999|\n","|     1|    306|   5.0|944248888|1999|\n","|     1|    307|   5.0|944253207|1999|\n","|     1|    322|   4.0|944053801|1999|\n","+------+-------+------+---------+----+\n","only showing top 20 rows\n","\n"]},{"data":{"text/plain":["spark = org.apache.spark.sql.SparkSession@40930058\n","ratingsPath = gs://deva_vasadi/movie_lens_data/ratings.csv\n","ratingsDF = [userId: string, movieId: string ... 2 more fields]\n","ratingsWithYearDF = [userId: string, movieId: string ... 3 more fields]\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["[userId: string, movieId: string ... 3 more fields]"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["import org.apache.spark.sql.SparkSession\n","import org.apache.spark.sql.functions._\n","import java.time.Instant\n","import java.time.ZoneId\n","import java.time.format.DateTimeFormatter\n","\n","// Step 1: Initialize SparkSession\n","val spark = SparkSession.builder()\n","  .appName(\"Time-Based Data Partitioning for Ratings\")\n","  .getOrCreate()\n","\n","// Step 2: Load ratings.csv as a DataFrame from GCP\n","val ratingsPath = \"gs://deva_vasadi/movie_lens_data/ratings.csv\"\n","val ratingsDF = spark.read.option(\"header\", \"true\").csv(ratingsPath)\n","\n","// Step 3: Convert the timestamp field to a year column\n","val ratingsWithYearDF = ratingsDF.withColumn(\n","  \"year\",\n","  year(from_unixtime(col(\"timestamp\").cast(\"long\")))\n",")\n","ratingsWithYearDF.show()"]},{"cell_type":"code","execution_count":14,"id":"7b910b93","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Saving...\n","Program complete\n"]},{"data":{"text/plain":["validRatingsDF = [userId: string, movieId: string ... 3 more fields]\n","outputPath = hdfs:///user/casestudies/casestudy5\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["hdfs:///user/casestudies/casestudy5"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["// Step 4: Transformation - Filter out invalid or incomplete records\n","val validRatingsDF = ratingsWithYearDF\n","  .filter(col(\"userId\").isNotNull && col(\"movieId\").isNotNull && col(\"rating\").isNotNull && col(\"timestamp\").isNotNull)\n","\n","// Step 5: Save Data Partitioned by UserId to HDFS\n","println(\"Saving...\")\n","\n","val outputPath = \"hdfs:///user/casestudies/casestudy5\"\n","validRatingsDF.limit(1000000).write\n","    .partitionBy(\"year\")\n","    .format(\"parquet\").mode(\"overwrite\").save(outputPath)\n","\n","println(\"Program complete\")"]},{"cell_type":"code","execution_count":null,"id":"aa057e55","metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Apache Toree - Scala","language":"scala","name":"apache_toree_scala"},"language_info":{"codemirror_mode":"text/x-scala","file_extension":".scala","mimetype":"text/x-scala","name":"scala","pygments_lexer":"scala","version":"2.12.15"}},"nbformat":4,"nbformat_minor":5}